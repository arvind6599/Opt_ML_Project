{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Leakage from Gradients.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvind6599/Opt_ML_Project/blob/main/Quantitative_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWa7Xo6PkIl3",
        "outputId": "dd197611-dd3d-4455-a427-98722689225a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "import torchvision\n",
        "from torchvision import models, datasets, transforms\n",
        "torch.manual_seed(50)\n",
        "\n",
        "print(torch.__version__, torchvision.__version__)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121 0.18.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKWqs2akepH",
        "outputId": "6f45ee62-41b0-4004-b675-4cff46d04f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dst = datasets.CIFAR100(\"~/.torch\", download=True)\n",
        "tp = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "tt = transforms.ToPILImage()\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "print(\"Running on %s\" % device)\n",
        "\n",
        "def label_to_onehot(target, num_classes=100):\n",
        "    target = torch.unsqueeze(target, 1)\n",
        "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
        "    onehot_target.scatter_(1, target, 1)\n",
        "    return onehot_target\n",
        "\n",
        "def cross_entropy_for_onehot(pred, target):\n",
        "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AorI020iVjjS"
      },
      "source": [
        "def weights_init(m):\n",
        "    if hasattr(m, \"weight\"):\n",
        "        m.weight.data.uniform_(-0.5, 0.5)\n",
        "    if hasattr(m, \"bias\"):\n",
        "        m.bias.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(3, 12, kernel_size=5, padding=5//2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
        "            act(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(768, 100)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        # print(out.size())\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize(x,input_compress_settings={}):\n",
        "    compress_settings={'n':6}\n",
        "    compress_settings.update(input_compress_settings)\n",
        "    #assume that x is a torch tensor\n",
        "\n",
        "    n=compress_settings['n']\n",
        "    #print('n:{}'.format(n))\n",
        "    x=x.float()\n",
        "    x_norm=torch.norm(x,p=float('inf'))\n",
        "\n",
        "    sgn_x=((x>0).float()-0.5)*2\n",
        "\n",
        "    p=torch.div(torch.abs(x),x_norm)\n",
        "    renormalize_p=torch.mul(p,n)\n",
        "    floor_p=torch.floor(renormalize_p)\n",
        "    compare=torch.rand_like(floor_p)\n",
        "    final_p=renormalize_p-floor_p\n",
        "    margin=(compare < final_p).float()\n",
        "    xi=(floor_p+margin)/n\n",
        "\n",
        "    Tilde_x=x_norm*sgn_x*xi\n",
        "\n",
        "    return Tilde_x\n",
        "\n",
        "\n",
        "def uniform_quantization(x, levels=16):\n",
        "    '''\n",
        "    Perform uniform quantization on the input tensor x, with levels number of levels.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The input tensor to be quantized.\n",
        "    levels (int): The number of levels to quantize the input tensor into.\n",
        "\n",
        "    '''\n",
        "    min_val, max_val = x.min(), x.max()\n",
        "    scale = (max_val - min_val) / (levels - 1)\n",
        "    quantized = torch.round((x - min_val) / scale) * scale + min_val\n",
        "    return quantized\n",
        "\n",
        "def log_quantization(tensor, base=2):\n",
        "    '''\n",
        "    Perform log quantization on the input tensor x, with base as the base of the logarithm.\n",
        "\n",
        "    Parameters:\n",
        "    tensor (torch.Tensor): The input tensor to be quantized.\n",
        "    base (int): The base of the logarithm to be used for quantization.\n",
        "    '''\n",
        "\n",
        "    sign = torch.sign(tensor)\n",
        "    log_tensor = torch.log(torch.abs(tensor) + 1e-9) / torch.log(torch.tensor(base))\n",
        "    quantized = torch.round(log_tensor) * torch.log(torch.tensor(base))\n",
        "    return sign * torch.exp(quantized)\n",
        "\n",
        "\n",
        "\n",
        "def kmeans_quantization(tensor, clusters=4):\n",
        "    '''\n",
        "    Perform k-means quantization on the input tensor x, with clusters number of clusters.\n",
        "    Parameters:\n",
        "    tensor (torch.Tensor): The input tensor to be quantized.\n",
        "    clusters (int): The number of clusters to quantize the input tensor into.\n",
        "    '''\n",
        "    tensor_reshaped = tensor.view(-1, 1).numpy()\n",
        "    kmeans = KMeans(n_clusters=clusters).fit(tensor_reshaped)\n",
        "    quantized = torch.tensor(kmeans.cluster_centers_[kmeans.labels_]).view_as(tensor)\n",
        "    return quantized\n",
        "\n",
        "\n",
        "def stochastic_rounding(tensor, levels=16):\n",
        "    '''\n",
        "    Stochastic rounding involves rounding to the nearest quantized value with a probability proportional to the distance from the exact value, which can preserve more information in expectation\n",
        "\n",
        "    Parameters:\n",
        "    tensor (torch.Tensor): The input tensor to be quantized.\n",
        "    levels (int): The number of levels to quantize the input tensor into.\n",
        "    '''\n",
        "\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (levels - 1)\n",
        "    scaled = (tensor - min_val) / scale\n",
        "    lower = torch.floor(scaled)\n",
        "    upper = torch.ceil(scaled)\n",
        "    prob = scaled - lower\n",
        "    quantized = torch.where(torch.rand_like(tensor) < prob, upper, lower) * scale + min_val\n",
        "    return quantized\n",
        "\n",
        "def fixed_point_quantization(tensor, num_bits, fractional_bits):\n",
        "    '''\n",
        "    Fixed-point quantization involves scaling the input tensor by a power of 2, rounding to the nearest integer, and then scaling back to the original range.\n",
        "    '''\n",
        "\n",
        "    scale = 2 ** fractional_bits\n",
        "    quantized = torch.round(tensor * scale) / scale\n",
        "    max_val = 2 ** (num_bits - fractional_bits - 1) - 1 / scale\n",
        "    min_val = -max_val\n",
        "    quantized = torch.clamp(quantized, min_val, max_val)\n",
        "    return quantized\n",
        "\n",
        "\n",
        "def add_sparsity(x, sparsity_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Adds sparsity to the input tensor by setting a specified percentage of the smallest absolute values to zero.\n",
        "\n",
        "    Parameters:\n",
        "    tensor (torch.Tensor): The input tensor.\n",
        "    sparsity_ratio (float): The ratio of elements to be set to zero, between 0 and 1.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The sparse tensor.\n",
        "    \"\"\"\n",
        "    flat_tensor = x.flatten()\n",
        "    k = int(sparsity_ratio * flat_tensor.size(0))\n",
        "\n",
        "    # print(\"Number of elemnts that will be zeroed out\",k)\n",
        "\n",
        "    if k > 0:\n",
        "        threshold = flat_tensor.abs().kthvalue(k).values.item()\n",
        "        mask = flat_tensor.abs() > threshold\n",
        "        sparse_tensor = flat_tensor * mask.float()\n",
        "        return sparse_tensor.view_as(x)\n",
        "    else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "_4d8hjH2H_ll"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZNuuwTFlYr0",
        "outputId": "4bc70d63-7c01-4c2e-b9fa-bb4843fdd83c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "NUM_SEEDS = 10\n",
        "\n",
        "loss_histories = []\n",
        "pbar = tqdm(range(NUM_SEEDS))\n",
        "for _ in pbar:\n",
        "    net = LeNet().to(device)\n",
        "\n",
        "    net.apply(weights_init)\n",
        "    criterion = cross_entropy_for_onehot\n",
        "\n",
        "    ######### honest partipant #########\n",
        "    img_index = 25\n",
        "    gt_data = tp(dst[img_index][0]).to(device)\n",
        "    gt_data = gt_data.view(1, *gt_data.size())\n",
        "    gt_label = torch.Tensor([dst[img_index][1]]).long().to(device)\n",
        "    gt_label = gt_label.view(1, )\n",
        "    gt_onehot_label = label_to_onehot(gt_label, num_classes=100)\n",
        "\n",
        "    # compute original gradient\n",
        "    out = net(gt_data)\n",
        "    y = criterion(out, gt_onehot_label)\n",
        "    dy_dx = torch.autograd.grad(y, net.parameters())\n",
        "\n",
        "\n",
        "    # share the gradients with other clients\n",
        "    original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
        "\n",
        "    # generate dummy data and label\n",
        "    dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
        "    dummy_label = torch.randn(gt_onehot_label.size()).to(device).requires_grad_(True)\n",
        "\n",
        "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label] )\n",
        "\n",
        "    history = []\n",
        "    loss_history = []\n",
        "    for iters in range(100):\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = net(dummy_data)\n",
        "            dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n",
        "            dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
        "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "\n",
        "            grad_diff = 0\n",
        "            grad_count = 0\n",
        "            for gx, gy_ in zip(dummy_dy_dx, original_dy_dx): # TODO: fix the variablas here\n",
        "                gy = add_sparsity(gy_, 0.4)\n",
        "                grad_diff += ((gx - gy) ** 2).sum()\n",
        "                grad_count += gx.nelement()\n",
        "            # grad_diff = grad_diff / grad_count * 1000\n",
        "            grad_diff.backward()\n",
        "\n",
        "            return grad_diff\n",
        "\n",
        "        optimizer.step(closure)\n",
        "        if iters % 10 == 0:\n",
        "            current_loss = closure()\n",
        "            #print(iters, \"%.4f\" % current_loss.item())\n",
        "        pbar.set_description(f\"Iterations: {iters}/100 | Loss: {current_loss.item()}\")\n",
        "        loss_history.append(current_loss.item())\n",
        "        history.append(tt(dummy_data[0].cpu()))\n",
        "    loss_histories.append(loss_history)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iterations: 99/100 | Loss: 1.3206108808517456: 100%|██████████| 10/10 [07:46<00:00, 46.63s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(np.mean([x[-1] for x in loss_histories]))\n",
        "print(np.median([x[-1] for x in loss_histories]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4d7975wKckA",
        "outputId": "8218cded-5141-4a7b-834e-b3e96fd3dfd2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0902176022529604\n",
            "2.956841826438904\n"
          ]
        }
      ]
    }
  ]
}